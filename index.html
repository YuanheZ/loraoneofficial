<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LoRA-One: An Interactive Explainer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Visualization & Content Choices:
        - Report Info: LoRA vs LoRA-One subspace alignment (Fig 1b, 2). Goal: Compare optimization paths. Viz: Interactive Line Chart (Chart.js). Interaction: Toggle buttons. Justification: Visually proves core claim.
        - Report Info: Performance across tasks (Tables 2, 3). Goal: Compare performance. Viz: Interactive Bar Chart (Chart.js). Interaction: Dropdown menu. Justification: Digestible data exploration.
        - Report Info: Scalability (Fig 5). Goal: Show change over time. Viz: Line Chart (Chart.js). Justification: Illustrates trends.
        - Report Info: Algorithm 1. Goal: Inform. Viz: Styled HTML code block. Justification: Readable algorithm.
        - Report Info: Theoretical Results (Abstract, Intro, Theory sections). Goal: Inform. Viz: Formatted text with bullet points. Justification: Clear presentation of core proofs.
        - Library/Method: All charts use Chart.js on <canvas>. Diagrams: HTML/Tailwind. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #fdfdfc;
            color: #374151;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 700px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 45vh;
        }
        @media (max-width: 768px) {
            .chart-container {
                height: 300px;
                max-height: 50vh;
            }
        }
        .section-title {
            font-size: 1.875rem;
            font-weight: 600;
            line-height: 2.25rem;
            color: #111827;
            text-align: center;
            margin-bottom: 1rem;
        }
        .section-intro {
            text-align: center;
            max-width: 48rem;
            margin: 0 auto 3rem auto;
            color: #4B5563;
            line-height: 1.75;
        }
        .abstract-container {
            max-width: 700px; /* or whatever looks good */
            margin: 0 auto; /* auto margins center the block */
            padding: 0 20px; /* small side padding for safety on narrow screens */
        }
        .btn {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            font-weight: 500;
            transition: all 0.2s ease-in-out;
            cursor: pointer;
        }
        .btn-primary {
            background-color: #3b82f6;
            color: white;
        }
        .btn-primary:hover {
            background-color: #2563eb;
        }
        .btn-secondary {
            background-color: #e5e7eb;
            color: #374151;
        }
        .btn-secondary.active {
            background-color: #1e40af;
            color: white;
        }
        .theory-card {
            background-color: white;
            padding: 1.5rem;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1), 0 2px 4px -1px rgba(0,0,0,0.06);
            border: 1px solid #e5e7eb;
        }
        .theory-card h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: #1e40af; /* Deep blue for theory titles */
            margin-bottom: 0.75rem;
        }
        .theory-card p {
            color: #4B5563;
            line-height: 1.65;
        }
    </style>
</head>
<body class="antialiased">

    <main class="w-full">
        <section id="hero" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-4xl mx-auto text-center">
                <h1 class="text-4xl md:text-5xl font-bold text-gray-900 mb-4">LoRA-One</h1>
                <p class="text-xl md:text-2xl text-gray-600 mb-6">One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</p>
                <div class="text-sm text-gray-500 mb-8">
                    <p>Yuanhe Zhang¹, Fanghui Liu¹, Yudong Chen²</p>
                    <p class="italic">¹University of Warwick, ²University of Wisconsin-Madison</p>
                    <p class="mt-2 font-semibold text-gray-600">ICML 2025 Oral</p>
                </div>
                <div class="flex justify-center items-center space-x-4">
                    <a href="#results" class="btn btn-primary">Explore Results</a>
                    <a href="https://arxiv.org/abs/2502.01235" target="_blank" class="btn btn-secondary">Read Paper</a>
                    <a href="https://github.com/YuanheZ/LoRA-One" target="_blank" class="btn btn-secondary">View Code</a>
                </div>
            </div>
        </section>

        <section id="challenge" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Abstract</h2>
                <p class="section-intro">
                    This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation.
                </p>
                <div class="bg-white p-8 rounded-lg shadow-md border border-gray-200 grid md:grid-cols-2 gap-8 items-center">
                    <div>
                        <h3 class="text-xl font-semibold text-gray-800 mb-3">Standard LoRA: The Foundation</h3>
                        <p class="text-gray-600 leading-relaxed">LoRA freezes <img src="http://latex.codecogs.com/svg.latex?\{\mathbf{x}_i\in\mathbb{R}^d\}_{i=1}^N" alt="\{\mathbf{x}_i\in\mathbb{R}^d\}_{i=1}^N" border="0"/> the massive pre-trained weights of an LLM and injects small, trainable "adapter" matrices ($A$ and $B$) into each layer. Only these adapters are trained, drastically reducing the number of trainable parameters. The core idea is that the necessary "update" to the original weights for a new task can be approximated by a low-rank matrix, which is the product of $A$ and $B$.</p>
                        <p class="text-gray-600 leading-relaxed mt-4">However, standard LoRA typically starts with a random initialization, meaning it can take many training steps to learn the correct "direction" for the adaptation.</p>
                    </div>
                    <div class="flex flex-col items-center justify-center p-6 bg-blue-50 rounded-lg">
                        <div class="w-full h-24 bg-blue-200 rounded-md flex items-center justify-center text-blue-800 font-bold text-lg shadow-inner">Frozen Pre-trained Weights (W)</div>
                        <div class="text-4xl font-thin text-blue-400 my-2">+</div>
                        <div class="flex items-center space-x-2">
                             <div class="w-20 h-24 bg-green-200 rounded-md flex items-center justify-center text-green-800 font-bold shadow-inner">A</div>
                             <div class="text-2xl text-green-600">×</div>
                             <div class="w-24 h-20 bg-green-200 rounded-md flex items-center justify-center text-green-800 font-bold shadow-inner">B</div>
                        </div>
                        <p class="mt-4 text-sm text-gray-500 text-center">Only the small, trainable adapters (A, B) are updated.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="insight" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">The Core Insight: Subspace Alignment</h2>
                <p class="section-intro">
                    This is the central theoretical motivation of LoRA-One. We prove that vanilla LoRA will align with the most important directions (the top singular subspaces) of the first-step gradient from full fine-tuning. Under mild conditions, in the linear setting, LoRA via Gradient Descent (GD) yields
                      <center>
                        <img src="http://latex.codecogs.com/svg.latex?
                             \textit{Primal:}\, 
                             \begin{cases}
                                e(\mathbf{x}) = W_{e|X}^\top \phi_q(\mathbf{x})
                                \\
                                r(\mathbf{x}) = W_{r|X}^\top \phi_k(\mathbf{x})
                             \end{cases},
                             \quad
                             \textit{Dual:}\, 
                             \begin{cases}
                                e(\mathbf{x}) = \sum\nolimits_{j=1}^N \mathbf{h}_{r_j} \kappa(\mathbf{x},\mathbf{x}_j) 
                                \\
                                r(\mathbf{x}) = \sum\nolimits_{i=1}^N \mathbf{h}_{e_i} \kappa(\mathbf{x}_i,\mathbf{x}) 
                             \end{cases}.
                             " alt="" border="0"/>
                      </center>
                </p>
                <center>
                        <img src="./img/mathchalora5.png" width="600">
                </center>
            </div>
        </section>

        <section id="method" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">The LoRA-One Method</h2>
                <p class="section-intro">
                    LoRA-One's effectiveness stems from its principled initialization strategy. Instead of a random start, it computes the gradient of the loss with respect to the pre-trained weights for the new task. It then performs a Singular Value Decomposition (SVD) on this gradient to find the most important directional information and uses this to construct the initial adapter matrices $A_0$ and $B_0$. This "Spectral Initialization" is the core of the algorithm.
                </p>
                 <div class="bg-gray-800 text-white p-6 rounded-lg shadow-lg font-mono text-sm max-w-3xl mx-auto">
                    <h3 class="text-lg font-semibold text-green-400 mb-4">// Algorithm 1: LoRA-One (Simplified)</h3>
                    <p><span class="text-purple-400">Input:</span> Pre-trained weight $W_{pre}$, data $D$, rank $r$</p>
                    <p class="mt-4 text-green-400 font-bold">Initialize:</p>
                    <ol class="list-decimal list-inside pl-4 mt-2 space-y-1">
                        <li>Compute one-step full gradient: $G = \nabla_W \mathcal{L}(W_{pre}, D)$</li>
                        <li>Perform SVD on the gradient: $U, S, V = \text{SVD}(G)$</li>
                        <li><span class="text-pink-400 font-bold">Spectral-Init:</span> Construct $A_0$ from $U, S$ and $B_0$ from $S, V$</li>
                        <li>Free memory used by the full gradient $G$</li>
                    </ol>
                     <p class="mt-4 text-green-400 font-bold">Train:</p>
                     <ol class="list-decimal list-inside pl-4 mt-2 space-y-1" start="5">
                        <li>For each training step:</li>
                        <li>  Update only $A_t$ and $B_t$ using their gradients</li>
                     </ol>
                    <p class="mt-4"><span class="text-purple-400">Return:</span> $W_{pre} + A_{final} \cdot B_{final}$</p>
                </div>
            </div>
        </section>

        <section id="theory" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Key Theoretical Underpinnings</h2>
                <p class="section-intro">
                    LoRA-One is not just an empirical success; it's built on a solid theoretical foundation. The paper provides rigorous proofs that explain why LoRA-One works and how it improves upon existing methods. Here are some of the core theoretical contributions:
                </p>
                <div class="grid md:grid-cols-2 gap-6">
                    <div class="theory-card">
                        <h3>1. Subspace Alignment Dynamics</h3>
                        <p>The theory first establishes that standard LoRA adapters, when trained with gradient descent, gradually align with the top-$r$ singular subspaces of the one-step full fine-tuning gradient, denoted as $\nabla_W \mathcal{L}(W_{pre}, D)$. This alignment is crucial for effective adaptation.</p>
                    </div>
                    <div class="theory-card">
                        <h3>2. Immediate Alignment via Spectral Initialization</h3>
                        <p>Building on the first point, LoRA-One's "Spectral Initialization" (using SVD of $\nabla_W \mathcal{L}$) is proven to achieve this optimal subspace alignment instantaneously at $t=0$. This gives LoRA-One a significant advantage by starting the fine-tuning process in the most promising directions.</p>
                    </div>
                    <div class="theory-card">
                        <h3>3. Provable Linear Convergence</h3>
                        <p>LoRA-One comes with theoretical guarantees of linear convergence. This means the method is proven to approach the optimal solution at a geometric rate, ensuring efficient and reliable training. This applies to both linear models and, under certain conditions, nonlinear models like Transformers.</p>
                    </div>
                    <div class="theory-card">
                        <h3>4. Generalization Bounds</h3>
                        <p>Beyond convergence, the theory provides generalization bounds for LoRA-One. These bounds help explain why LoRA-One not only trains well but also performs robustly on unseen data, which is critical for real-world applications.</p>
                    </div>
                    <div class="theory-card">
                        <h3>5. Mitigating Ill-Conditioning</h3>
                        <p>The paper theoretically analyzes how ill-conditioning in the gradient can affect LoRA-based methods. It shows that incorporating preconditioners (e.g., based on the Hessian or Fisher information matrix) can provably mitigate these effects, leading to more stable and faster convergence for LoRA-One.</p>
                    </div>
                    <div class="theory-card">
                        <h3>6. Unifying Perspectives</h3>
                        <p>The theoretical framework developed for LoRA-One also helps to unify and clarify the connections between LoRA-One and other gradient-alignment-based PEFT methods. This provides a clearer understanding of the landscape of efficient fine-tuning techniques.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="results" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Experimental Results Dashboard</h2>
                <p class="section-intro">
                    Theoretical advantages are compelling, but empirical results are crucial. LoRA-One consistently outperforms standard LoRA and other variants across a wide range of tasks and models. Select a task category below to explore the performance data from the paper. The charts show LoRA-One's improvements on established benchmarks.
                </p>
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-md border border-gray-200">
                    <div class="flex flex-col md:flex-row items-center justify-center mb-6">
                        <label for="task-selector" class="mr-3 font-medium text-gray-700">Select Task Category:</label>
                        <select id="task-selector" class="p-2 border border-gray-300 rounded-md shadow-sm focus:ring-blue-500 focus:border-blue-500">
                            <option value="nlu">Natural Language Understanding (T5)</option>
                            <option value="math">Mathematical Reasoning (LLaMA2-7B)</option>
                            <option value="code">Code Generation (LLaMA2-7B)</option>
                        </select>
                    </div>
                    <div class="grid md:grid-cols-5 gap-8 items-center">
                        <div class="md:col-span-3 chart-container h-[400px] md:h-[350px]">
                            <canvas id="resultsChart"></canvas>
                        </div>
                        <div class="md:col-span-2">
                            <h3 id="results-title" class="text-xl font-semibold text-gray-800 mb-3"></h3>
                            <p id="results-text" class="text-gray-600 leading-relaxed"></p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="scalability" class="py-16 md:py-24 px-4 bg-gray-50">
             <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Scalability and Efficiency</h2>
                <p class="section-intro">
                    A key question for any new method is how it performs with more data and whether it introduces significant computational overhead. The experiments show that LoRA-One's advantages hold up during longer training, and it achieves this superior performance with virtually no extra time or memory cost during the fine-tuning phase itself.
                </p>
                <div class="grid md:grid-cols-2 gap-8">
                    <div class="bg-white p-6 rounded-lg shadow-md border border-gray-200">
                         <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Scaling with More Data and Epochs</h3>
                         <div class="chart-container h-80">
                            <canvas id="scalabilityChart"></canvas>
                         </div>
                         <p class="text-center mt-4 text-sm text-gray-600">Accuracy on GSM8K when fine-tuning on the full MetaMathQA dataset. LoRA-One consistently leads over multiple epochs.</p>
                    </div>
                     <div class="bg-white p-6 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Time & Memory Efficiency</h3>
                        <div class="overflow-x-auto">
                           <table class="w-full text-left text-sm">
                                <thead>
                                    <tr class="border-b bg-gray-50">
                                        <th class="p-3 font-medium">Dataset</th>
                                        <th class="p-3 font-medium">LoRA</th>
                                        <th class="p-3 font-medium text-blue-600">LoRA-One</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="border-b">
                                        <td class="p-3 font-medium">MetaMathQA</td>
                                        <td class="p-3">6h 20m (21.6GB)</td>
                                        <td class="p-3">6h 23m (21.7GB)</td>
                                    </tr>
                                     <tr class="border-b">
                                        <td class="p-3 font-medium">Alpaca</td>
                                        <td class="p-3">3h 22m (23.4GB)</td>
                                        <td class="p-3">3h 25m (23.4GB)</td>
                                    </tr>
                                    <tr>
                                        <td class="p-3 font-medium">Code-Feedback</td>
                                        <td class="p-3">6h 24m (22.6GB)</td>
                                        <td class="p-3">6h 26m (22.9GB)</td>
                                    </tr>
                                </tbody>
                           </table>
                        </div>
                         <p class="text-center mt-4 text-sm text-gray-600">The one-time cost of Spectral Initialization is negligible, making LoRA-One's runtime and memory footprint nearly identical to standard LoRA.</p>
                    </div>
                </div>
            </div>
        </section>

        <footer class="py-16 px-4 bg-gray-800 text-gray-300">
            <div class="max-w-4xl mx-auto text-center">
                 <h2 class="text-2xl font-semibold text-white mb-4">Conclusion & Citation</h2>
                 <p class="max-w-3xl mx-auto leading-relaxed mb-8">LoRA-One provides a more effective, efficient, and stable method for fine-tuning LLMs, backed by rigorous theory. By achieving immediate subspace alignment, it sets a new standard for parameter-efficient adaptation.</p>
                 <div class="bg-gray-900 text-left text-sm font-mono p-6 rounded-lg">
                    <pre><code>@inproceedings{zhang2025loraone,
  title={{LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, 
          Provably and Efficiently}},
  author={Zhang, Yuanhe and Liu, Fanghui and Chen, Yudong},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2025}
}</code></pre>
                 </div>
            </div>
        </footer>

    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            let alignmentChartInstance;
            let resultsChartInstance;
            let scalabilityChartInstance;

            const chartColors = {
                primary: 'rgba(59, 130, 246, 0.8)',
                primaryBorder: 'rgba(59, 130, 246, 1)',
                secondary: 'rgba(20, 184, 166, 0.8)',
                secondaryBorder: 'rgba(20, 184, 166, 1)',
                tertiary: 'rgba(107, 114, 128, 0.5)',
                tertiaryBorder: 'rgba(107, 114, 128, 1)',
                quaternary: 'rgba(102, 51, 204, 0.7)',
                quaternaryBorder: 'rgba(102, 51, 204, 1)',
            };
            
            const alignmentData = {
                lora: {
                    labels: Array.from({ length: 11 }, (_, i) => {
                        if (i === 9) return '...';
                        if (i === 10) return 't*≈log(input_dim)';
                        return `t=${i}`;
                        }),
                    data: [1.0, 0.9, 0.75, 0.55, 0.3, 0.15, 0.08, 0.04, 0.02, 0.01, 0],
                    text: 'Standard LoRA gradually learns the correct subspace, with the alignment angle decreasing over many training steps.',
                    color: chartColors.secondary
                },
                loraOne: {
                    labels: Array.from({ length: 11 }, (_, i) => `t=${i}`),
                    data: Array(11).fill(0),
                    text: "LoRA-One's Spectral Initialization aligns the adapters with the optimal subspace at iteration 0, giving it a significant head start.",
                    color: chartColors.primary
                }
            };
            
            const resultsData = {
                nlu: {
                    title: "Natural Language Understanding (GLUE)",
                    text: "On the GLUE benchmark with a T5-base model, LoRA-One shows significant improvements, particularly on tasks like COLA and MRPC, increasing the average accuracy by over 6 points compared to standard LoRA.",
                    labels: ["MNLI", "SST-2", "COLA", "QNLI", "MRPC"],
                    datasets: [
                        { label: 'LoRA', data: [85.30, 94.04, 72.84, 93.02, 68.38], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [85.89, 94.53, 82.04, 93.37, 87.83], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                },
                math: {
                    title: "Mathematical Reasoning (GSM8K)",
                    text: "When fine-tuning LLaMA 2-7B for mathematical reasoning, LoRA-One outperforms both LoRA and LoRA-GA, showing improved problem-solving accuracy in both direct and Chain-of-Thought (CoT) prompting.",
                    labels: ["GSM8K Direct", "GSM8K CoT"],
                    datasets: [
                        { label: 'LoRA', data: [59.26, 53.36], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-GA', data: [56.44, 45.15], backgroundColor: chartColors.tertiary, borderColor: chartColors.tertiaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [60.44, 55.88], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                },
                code: {
                    title: "Code Generation (HumanEval)",
                    text: "In code synthesis tasks, LoRA-One achieves a nearly 3-point higher PASS@1 score than standard LoRA, demonstrating its effectiveness in adapting models for structured, logical generation.",
                    labels: ["HumanEval PASS@1"],
                    datasets: [
                        { label: 'LoRA', data: [25.85], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-GA', data: [26.95], backgroundColor: chartColors.tertiary, borderColor: chartColors.tertiaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [28.66], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                }
            };
            
            const scalabilityData = {
                labels: ['Epoch 1', 'Epoch 2', 'Epoch 3', 'Epoch 4'],
                datasets: [
                     {
                        label: 'LoRA',
                        data: [55.19, 58.37, 59.28, 58.90],
                        borderColor: chartColors.secondaryBorder,
                        backgroundColor: chartColors.secondary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA+',
                        data: [56.37, 59.21, 59.93, 59.97],
                        borderColor: chartColors.quaternaryBorder,
                        backgroundColor: chartColors.quaternary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA-GA',
                        data: [56.48, 58.64, 60.16, 60.88],
                        borderColor: chartColors.tertiaryBorder,
                        backgroundColor: chartColors.tertiary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA-One',
                        data: [57.54, 60.84, 62.62, 63.80],
                        borderColor: chartColors.primaryBorder,
                        backgroundColor: chartColors.primary,
                        fill: false,
                        tension: 0.1
                    }
                ]
            };

            function createAlignmentChart(initialMode = 'loraOne') {
                const ctx = document.getElementById('alignmentChart').getContext('2d');
                const initialData = alignmentData[initialMode];
                
                if(alignmentChartInstance) alignmentChartInstance.destroy();

                alignmentChartInstance = new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: initialData.labels,
                        datasets: [{
                            label: 'Principal Angle to Optimal Subspace',
                            data: initialData.data,
                            borderColor: initialData.color,
                            backgroundColor: initialData.color.replace('1)', '0.2)'),
                            fill: true,
                            tension: 0.4
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: {
                            legend: { display: false },
                            title: { display: true, text: 'Subspace Alignment Over Training Iterations', font: { size: 16 } }
                        },
                        scales: {
                            y: {
                                beginAtZero: true,
                                suggestedMax: 1.1,
                                title: { display: true, text: 'Principal Angle (Lower is Better)' }
                            },
                            x: {
                                title: { display: true, text: 'Training Iteration' }
                            }
                        }
                    }
                });
                document.getElementById('alignment-text').textContent = initialData.text;
            }

            function updateAlignmentChart(mode) {
                const data = alignmentData[mode];
                alignmentChartInstance.data.labels = data.labels;
                alignmentChartInstance.data.datasets[0].data = data.data;
                alignmentChartInstance.data.datasets[0].borderColor = data.color;
                alignmentChartInstance.data.datasets[0].backgroundColor = data.color.replace('1)', '0.2)');
                alignmentChartInstance.update();
                document.getElementById('alignment-text').textContent = data.text;
                
                document.getElementById('show-lora').classList.toggle('active', mode === 'lora');
                document.getElementById('show-lora-one').classList.toggle('active', mode === 'loraOne');
            }
            
            function createResultsChart(category) {
                 const ctx = document.getElementById('resultsChart').getContext('2d');
                 const data = resultsData[category];

                 if(resultsChartInstance) resultsChartInstance.destroy();

                 resultsChartInstance = new Chart(ctx, {
                    type: 'bar',
                    data: {
                        labels: data.labels,
                        datasets: data.datasets
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        indexAxis: 'y',
                        scales: {
                            x: {
                                beginAtZero: true,
                                title: { display: true, text: 'Accuracy / Score (%)' }
                            }
                        },
                        plugins: {
                            legend: { position: 'bottom' },
                            title: { display: false },
                            tooltip: {
                                callbacks: {
                                    label: function(context) {
                                        return `${context.dataset.label}: ${context.raw}%`;
                                    }
                                }
                            }
                        }
                    }
                 });

                 document.getElementById('results-title').textContent = data.title;
                 document.getElementById('results-text').textContent = data.text;
            }

            function createScalabilityChart() {
                const ctx = document.getElementById('scalabilityChart').getContext('2d');
                scalabilityChartInstance = new Chart(ctx, {
                    type: 'line',
                    data: scalabilityData,
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                         plugins: {
                            legend: { position: 'bottom' },
                            title: { display: false },
                        },
                        scales: {
                            y: {
                                beginAtZero: false,
                                min: 54.5,
                                title: { display: true, text: 'Accuracy (%)' }
                            },
                             x: {
                                title: { display: true, text: 'Training Epoch' }
                            }
                        }
                    }
                });
            }

            document.getElementById('show-lora').addEventListener('click', () => updateAlignmentChart('lora'));
            document.getElementById('show-lora-one').addEventListener('click', () => updateAlignmentChart('loraOne'));
            document.getElementById('task-selector').addEventListener('change', (e) => createResultsChart(e.target.value));

            createAlignmentChart('loraOne');
            createResultsChart('nlu');
            createScalabilityChart();
        });
    </script>
</body>
</html>
