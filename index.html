<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LoRA-One: An Interactive Explainer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Visualization & Content Choices:
        - Report Info: LoRA vs LoRA-One subspace alignment (Fig 1b, 2). Goal: Compare optimization paths. Viz: Interactive Line Chart (Chart.js). Interaction: Toggle buttons. Justification: Visually proves core claim.
        - Report Info: Performance across tasks (Tables 2, 3). Goal: Compare performance. Viz: Interactive Bar Chart (Chart.js). Interaction: Dropdown menu. Justification: Digestible data exploration.
        - Report Info: Scalability (Fig 5). Goal: Show change over time. Viz: Line Chart (Chart.js). Justification: Illustrates trends.
        - Report Info: Algorithm 1. Goal: Inform. Viz: Styled HTML code block. Justification: Readable algorithm.
        - Report Info: Theoretical Results (Abstract, Intro, Theory sections). Goal: Inform. Viz: Formatted text with bullet points. Justification: Clear presentation of core proofs.
        - Library/Method: All charts use Chart.js on <canvas>. Diagrams: HTML/Tailwind. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #fdfdfc;
            color: #374151;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 700px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 45vh;
        }
        @media (max-width: 768px) {
            .chart-container {
                height: 300px;
                max-height: 50vh;
            }
        }
        .section-title {
            font-size: 1.875rem;
            font-weight: 600;
            line-height: 2.25rem;
            color: #111827;
            text-align: center;
            margin-bottom: 1rem;
        }
        .section-intro {
            text-align: center;
            max-width: 48rem;
            margin: 0 auto 3rem auto;
            color: #4B5563;
            line-height: 1.75;
        }
        .abstract-container {
            max-width: 700px; /* or whatever looks good */
            margin: 0 auto; /* auto margins center the block */
            padding: 0 20px; /* small side padding for safety on narrow screens */
        }
        .btn {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            font-weight: 500;
            transition: all 0.2s ease-in-out;
            cursor: pointer;
        }
        .btn-primary {
            background-color: #3b82f6;
            color: white;
        }
        .btn-primary:hover {
            background-color: #2563eb;
        }
        .btn-secondary {
            background-color: #e5e7eb;
            color: #374151;
        }
        .btn-secondary.active {
            background-color: #1e40af;
            color: white;
        }
        .theory-card {
            background-color: white;
            padding: 1.5rem;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1), 0 2px 4px -1px rgba(0,0,0,0.06);
            border: 1px solid #e5e7eb;
        }
        .theory-card h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: #1e40af; /* Deep blue for theory titles */
            margin-bottom: 0.75rem;
        }
        .theory-card p {
            color: #4B5563;
            line-height: 1.65;
        }
    </style>
</head>
<body class="antialiased">

    <main class="w-full">
        <section id="hero" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-4xl mx-auto text-center">
                <h1 class="text-4xl md:text-5xl font-bold text-gray-900 mb-4">LoRA-One</h1>
                <p class="text-xl md:text-2xl text-gray-600 mb-6">One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</p>
                <div class="text-sm text-gray-500 mb-8">
                    <p>
                        <a href="https://yuanhez.github.io" class="text-blue-600 hover:underline" target="_blank">Yuanhe Zhang</a>¹,
                        <a href="https://www.lfhsgre.org" class="text-blue-600 hover:underline" target="_blank">Fanghui Liu</a>¹,
                        <a href="https://pages.cs.wisc.edu/~yudongchen" class="text-blue-600 hover:underline" target="_blank">Yudong Chen</a>²
                    </p>
                    <p class="italic">¹University of Warwick, ²University of Wisconsin-Madison</p>
                    <p class="text-xl md:text-2xl text-gray-600 mb-6">ICML 2025 Oral</p>
                </div>
                <div class="flex justify-center items-center space-x-4">
                    <a href="#results" class="btn btn-primary">Explore Results</a>
                    <a href="https://arxiv.org/abs/2502.01235" target="_blank" class="btn btn-secondary">Read Paper</a>
                    <a href="https://github.com/YuanheZ/LoRA-One" target="_blank" class="btn btn-secondary">View Code</a>
                </div>
            </div>
        </section>

        <section id="challenge" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Abstract</h2>
                <p class="section-intro">
                    This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation.
                </p>
            </div>
        </section>

        <section id="insight" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">The Core Insight: Subspace Alignment</h2>
                <p class="section-intro  text-lg text-gray-700 mb-6">
                    This is the central theoretical motivation of LoRA-One. We prove that vanilla LoRA will align with the top singular subspaces of the first-step gradient from full fine-tuning. Under mild conditions, in the linear setting, LoRA via Gradient Descent (GD) yields
                </p>
                <center>
                    <img src="http://latex.codecogs.com/svg.latex?
                            \|\mathbf{V}_{r^*,\perp}^{\top}(\nabla_{\mathbf{W}} L(\mathbf{W}^{\natural})) \mathbf{V}_{r^*}(\mathbf{B}_{t})\|_{op}=0\,, \quad \forall\,t\geq 1\,,
                            " alt="" border="0" class="mt-6"/>
                    <img src="http://latex.codecogs.com/svg.latex?
                            \|\mathbf{U}_{r^*,\perp}^{\top}(\nabla_{\mathbf{W}} L(\mathbf{W}^{\natural})) \mathbf{U}_{r^*}(\mathbf{A}_{t^*})\|_{op}\lesssim \theta\,, \quad \text{for }\,t^* = \Theta(\log d)\,.
                            " alt="" border="0" class="mt-6"/>
                </center>
                <center>
                        <img src="./img/mathchalora5.png" width="600" class="mt-6">
                </center>
            </div>
        </section>

        <section id="method" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Methodology</h2>
                <p class="section-intro">
                    The subspace alignment motivates us to initialize LoRA using the best rank-r approximation of the first-step gradient from full fine-tuning. The "Spectral Initialization" is the core of the algorithm.
                </p>
                <center>
                    <img src="./img/alg-lora-one.png" width="600" class="mt-6">
                </center>
            </div>
        </section>

        <section id="theory" class="py-16 md:py-24 px-4 bg-gray-50">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Key Theoretical Underpinnings</h2>
                <div class="grid md:grid-cols-2 gap-6">
                    <div class="theory-card">
                        <h3>1. Subspace Alignment Dynamics</h3>
                        <p>The theory first establishes that standard LoRA adapters, when trained with gradient descent, gradually align with the target singular subspaces of the one-step full fine-tuning gradient. This alignment is crucial for effective adaptation.</p>
                    </div>
                    <div class="theory-card">
                        <h3>2. Immediate Alignment via Spectral Initialization</h3>
                        <p>Building on the first point, LoRA-One's "Spectral Initialization" is proven to achieve this optimal subspace alignment instantaneously at t=0. This gives LoRA-One a significant advantage by starting the fine-tuning process in the most promising directions.</p>
                    </div>
                    <div class="theory-card">
                        <h3>3. Provable Linear Convergence</h3>
                        <p>LoRA-One comes with theoretical guarantees of linear convergence. This means the method is proven to approach the optimal solution at a linear rate, ensuring efficient and reliable training. This applies to both linear models and, under certain conditions, nonlinear models like ReLU MLP.</p>
                    </div>
                    <div class="theory-card">
                        <h3>4. Generalization Bounds</h3>
                        <p>Beyond convergence, the theory provides generalization bounds for LoRA-One. These bounds help explain why LoRA-One not only trains well but also performs robustly on unseen data, which is critical for real-world applications.</p>
                    </div>
                    <div class="theory-card">
                        <h3>5. Mitigating Ill-Conditioning</h3>
                        <p>The paper theoretically analyzes how ill-conditioning in the downstream tasks can affect LoRA-based methods. It shows that incorporating preconditioners can provably mitigate these effects, leading to more stable and faster convergence for LoRA-One.</p>
                    </div>
                    <div class="theory-card">
                        <h3>6. Unifying Perspectives</h3>
                        <p>The theoretical framework developed for LoRA-One also helps to unify and clarify the connections between LoRA-One and other gradient-alignment-based PEFT methods. This provides a clearer understanding of the landscape of efficient fine-tuning techniques.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="results" class="py-16 md:py-24 px-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Experimental Results Dashboard</h2>
                <p class="section-intro">
                    Theoretical advantages are compelling, but empirical results are crucial. LoRA-One consistently outperforms standard LoRA and other variants across a wide range of tasks and models. Select a task category below to explore the performance data from the paper. The charts show LoRA-One's improvements on established benchmarks.
                </p>
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-md border border-gray-200">
                    <div class="flex flex-col md:flex-row items-center justify-center mb-6">
                        <label for="task-selector" class="mr-3 font-medium text-gray-700">Select Task Category:</label>
                        <select id="task-selector" class="p-2 border border-gray-300 rounded-md shadow-sm focus:ring-blue-500 focus:border-blue-500">
                            <option value="math">Mathematical Reasoning (LLaMA2-7B)</option>
                            <option value="code">Code Generation (LLaMA2-7B)</option>
                            <option value="inst">Instruction Tuning (LLaMA2-7B)</option>
                        </select>
                    </div>
                    <div class="grid md:grid-cols-5 gap-8 items-center">
                        <div class="md:col-span-3 chart-container h-[400px] md:h-[350px]">
                            <canvas id="resultsChart"></canvas>
                        </div>
                        <div class="md:col-span-2">
                            <h3 id="results-title" class="text-xl font-semibold text-gray-800 mb-3"></h3>
                            <p id="results-text" class="text-gray-600 leading-relaxed"></p>
                        </div>
                    </div>
                </div>
                <section id="nlu-chart" class="py-16 md:py-24 px-4 bg-gray-50">
                    <div class="max-w-5xl mx-auto">
                        <p class="section-intro">
                            Comparison with LoRA Variants on GLUE Benchmarks (T5 Base) - This chart summarizes performance of LoRA, LoRA-One, and other PEFT baselines across five natural language understanding tasks: MNLI, SST-2, CoLA, QNLI, and MRPC.
                        </p>
                        <div class="chart-container h-[400px] md:h-[350px]">
                            <canvas id="nluResultsChart"></canvas>
                        </div>
                    </div>
                </section>
            </div>
        </section>

        <section id="scalability" class="py-16 md:py-24 px-4 bg-gray-50">
             <div class="max-w-5xl mx-auto">
                <h2 class="section-title">Scalability and Efficiency</h2>
                <p class="section-intro">
                    The experiments show that LoRA-One's advantages hold up during longer training, and it achieves this superior performance with virtually no extra time or memory cost during the fine-tuning phase.
                </p>
                <div class="grid md:grid-cols-2 gap-8">
                    <div class="bg-white p-6 rounded-lg shadow-md border border-gray-200">
                         <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Scaling with More Data and Epochs</h3>
                         <div class="chart-container h-80">
                            <canvas id="scalabilityChart"></canvas>
                         </div>
                         <p class="text-center mt-4 text-sm text-gray-600">Accuracy on GSM8K when fine-tuning on the full MetaMathQA dataset. LoRA-One consistently leads over multiple epochs.</p>
                    </div>
                     <div class="bg-white p-6 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-xl font-semibold text-gray-800 mb-4 text-center">Time & Memory Efficiency</h3>
                        <div class="overflow-x-auto">
                           <table class="w-full text-left text-sm">
                                <thead>
                                    <tr class="border-b bg-gray-50">
                                        <th class="p-3 font-medium">Dataset</th>
                                        <th class="p-3 font-medium">LoRA</th>
                                        <th class="p-3 font-medium text-blue-600">LoRA-One</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="border-b">
                                        <td class="p-3 font-medium">MetaMathQA100K</td>
                                        <td class="p-3">6h 20m (21.6GB)</td>
                                        <td class="p-3">6h 23m (21.7GB)</td>
                                    </tr>
                                    <tr>
                                        <td class="p-3 font-medium">Code-Feedback100K</td>
                                        <td class="p-3">6h 24m (22.6GB)</td>
                                        <td class="p-3">6h 26m (22.9GB)</td>
                                    </tr>
                                     <tr class="border-b">
                                        <td class="p-3 font-medium">Alpaca</td>
                                        <td class="p-3">3h 22m (23.4GB)</td>
                                        <td class="p-3">3h 25m (23.4GB)</td>
                                    </tr>
                                </tbody>
                           </table>
                        </div>
                         <p class="text-center mt-4 text-sm text-gray-600">The time cost of spectral initialization is negligible due to sub-sampling, making LoRA-One's runtime and memory footprint nearly identical to standard LoRA.</p>
                    </div>
                </div>
            </div>
        </section>

        <footer class="py-16 px-4 bg-gray-800 text-gray-300">
            <div class="max-w-4xl mx-auto text-center">
                 <h2 class="text-2xl font-semibold text-white mb-4">Conclusion & Citation</h2>
                 <p class="max-w-3xl mx-auto leading-relaxed mb-8">LoRA-One provides a more effective, efficient, and stable method for fine-tuning LLMs, backed by rigorous theory. By achieving immediate subspace alignment, it sets a new standard for parameter-efficient adaptation.</p>
                 <div class="bg-gray-900 text-left text-sm font-mono p-6 rounded-lg">
                    <pre><code>@inproceedings{zhang2025loraone,
  title={{LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, 
          Provably and Efficiently}},
  author={Zhang, Yuanhe and Liu, Fanghui and Chen, Yudong},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2025}
}</code></pre>
                 </div>
            </div>
        </footer>

    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            let resultsChartInstance;
            let scalabilityChartInstance;

            const chartColors = {
                primary: 'rgba(59, 130, 246, 0.8)',
                primaryBorder: 'rgba(59, 130, 246, 1)',
                secondary: 'rgba(20, 184, 166, 0.8)',
                secondaryBorder: 'rgba(20, 184, 166, 1)',
                tertiary: 'rgba(107, 114, 128, 0.5)',
                tertiaryBorder: 'rgba(107, 114, 128, 1)',
                quaternary: 'rgba(102, 51, 204, 0.7)',
                quaternaryBorder: 'rgba(102, 51, 204, 1)',
            };
            
            const alignmentData = {
                lora: {
                    labels: Array.from({ length: 11 }, (_, i) => {
                        if (i === 9) return '...';
                        if (i === 10) return 't*≈log(input_dim)';
                        return `t=${i}`;
                        }),
                    data: [1.0, 0.9, 0.75, 0.55, 0.3, 0.15, 0.08, 0.04, 0.02, 0.01, 0],
                    text: 'Standard LoRA gradually learns the correct subspace, with the alignment angle decreasing over many training steps.',
                    color: chartColors.secondary
                },
                loraOne: {
                    labels: Array.from({ length: 11 }, (_, i) => `t=${i}`),
                    data: Array(11).fill(0),
                    text: "LoRA-One's Spectral Initialization aligns the adapters with the optimal subspace at iteration 0, giving it a significant head start.",
                    color: chartColors.primary
                }
            };
            
            const resultsData = {
                math: {
                    title: "Mathematical Reasoning (GSM8K)",
                    text: "After fine-tuning LLaMA 2-7B on MetaMathQA(100K) for mathematical reasoning, LoRA-One outperforms both LoRA and LoRA-GA on GSM8K, showing improved problem-solving accuracy in both direct and Chain-of-Thought (CoT) prompting.",
                    labels: ["Direct", "Few-Shot CoT"],
                    datasets: [
                        { label: 'LoRA', data: [59.26, 53.36], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-GA', data: [56.44, 45.15], backgroundColor: chartColors.tertiary, borderColor: chartColors.tertiaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [60.44, 55.88], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                },
                code: {
                    title: "Code Generation (HumanEval)",
                    text: "After fine-tuning LLaMA 2-7B on Code-Feedback(100K) for code synthesis tasks , LoRA-One achieves a nearly 3-point higher PASS@1 score than standard LoRA on HumanEval, demonstrating its effectiveness in adapting models for structured, logical generation.",
                    labels: ["PASS@1"],
                    datasets: [
                        { label: 'LoRA', data: [25.85], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-GA', data: [26.95], backgroundColor: chartColors.tertiary, borderColor: chartColors.tertiaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [28.66], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                },
                inst: {
                    title: "Instruction Tuning (MMLU)",
                    text: "After instruction fine-tuning LLaMA 2-7B on Alpaca, LoRA-One also shows superior generalization on the MMLU benchmark, indicating improved knowledge retention across diverse domains.",
                    labels: ["MMLU"],
                    datasets: [
                        { label: 'LoRA', data: [45.73], backgroundColor: chartColors.secondary, borderColor: chartColors.secondaryBorder, borderWidth: 1 },
                        { label: 'LoRA-GA', data: [45.15], backgroundColor: chartColors.tertiary, borderColor: chartColors.tertiaryBorder, borderWidth: 1 },
                        { label: 'LoRA-One', data: [47.24], backgroundColor: chartColors.primary, borderColor: chartColors.primaryBorder, borderWidth: 1 }
                    ]
                }
            };

            const nluChartData = {
                labels: ['MNLI', 'SST-2', 'CoLA', 'QNLI', 'MRPC'],
                datasets: [
                    {
                        label: 'LoRA',
                        data: [85.30, 94.04, 72.84, 93.02, 68.38],
                        backgroundColor: chartColors.secondary,
                        borderColor: chartColors.secondaryBorder,
                        borderWidth: 1
                    },
                    {
                        label: 'LoRA+',
                        data: [85.81, 93.85, 77.53, 93.14, 74.43],
                        backgroundColor: chartColors.quaternary,
                        borderColor: chartColors.quaternaryBorder,
                        borderWidth: 1
                    },
                    {
                        label: 'P-LoRA',
                        data: [85.28, 93.88, 79.58, 93.00, 83.91],
                        backgroundColor: '#fbbf24',
                        borderColor: '#f59e0b',
                        borderWidth: 1
                    },
                    {
                        label: 'PiSSA',
                        data: [85.75, 94.07, 74.27, 93.15, 76.31],
                        backgroundColor: '#10b981',
                        borderColor: '#059669',
                        borderWidth: 1
                    },
                    {
                        label: 'LoRA-GA',
                        data: [85.70, 94.11, 80.57, 93.18, 85.29],
                        backgroundColor: chartColors.tertiary,
                        borderColor: chartColors.tertiaryBorder,
                        borderWidth: 1
                    },
                    {
                        label: 'LoRA-Pro',
                        data: [86.03, 94.19, 81.94, 93.42, 86.60],
                        backgroundColor: '#ec4899',
                        borderColor: '#db2777',
                        borderWidth: 1
                    },
                    {
                        label: 'LoRA-One',
                        data: [85.89, 94.53, 82.04, 93.37, 87.83],
                        backgroundColor: chartColors.primary,
                        borderColor: chartColors.primaryBorder,
                        borderWidth: 1
                    }
                ]
            };
            
            const scalabilityData = {
                labels: ['Epoch 1', 'Epoch 2', 'Epoch 3', 'Epoch 4'],
                datasets: [
                     {
                        label: 'LoRA',
                        data: [55.19, 58.37, 59.28, 58.90],
                        borderColor: chartColors.secondaryBorder,
                        backgroundColor: chartColors.secondary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA+',
                        data: [56.37, 59.21, 59.93, 59.97],
                        borderColor: chartColors.quaternaryBorder,
                        backgroundColor: chartColors.quaternary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA-GA',
                        data: [56.48, 58.64, 60.16, 60.88],
                        borderColor: chartColors.tertiaryBorder,
                        backgroundColor: chartColors.tertiary,
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: 'LoRA-One',
                        data: [57.54, 60.84, 62.62, 63.80],
                        borderColor: chartColors.primaryBorder,
                        backgroundColor: chartColors.primary,
                        fill: false,
                        tension: 0.1
                    }
                ]
            };
            
            function createResultsChart(category) {
                 const ctx = document.getElementById('resultsChart').getContext('2d');
                 const data = resultsData[category];

                 if(resultsChartInstance) resultsChartInstance.destroy();

                 resultsChartInstance = new Chart(ctx, {
                    type: 'bar',
                    data: {
                        labels: data.labels,
                        datasets: data.datasets
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        indexAxis: 'y',
                        scales: {
                            x: {
                                beginAtZero: true,
                                title: { display: true, text: 'Accuracy / Score (%)' }
                            }
                        },
                        plugins: {
                            legend: { position: 'bottom' },
                            title: { display: false },
                            tooltip: {
                                callbacks: {
                                    label: function(context) {
                                        return `${context.dataset.label}: ${context.raw}%`;
                                    }
                                }
                            }
                        }
                    }
                 });

                 document.getElementById('results-title').textContent = data.title;
                 document.getElementById('results-text').textContent = data.text;
            }

            function createNLUChart() {
                const ctx = document.getElementById('nluResultsChart').getContext('2d');
                new Chart(ctx, {
                    type: 'bar',
                    data: nluChartData,
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: false,
                                min: 65,
                                max: 95,
                                title: { display: true, text: 'Accuracy (%)' }
                            }
                        },
                        plugins: {
                            legend: { position: 'bottom' },
                            tooltip: {
                                callbacks: {
                                    label: function(context) {
                                        return `${context.dataset.label}: ${context.raw}%`;
                                    }
                                }
                            }
                        }
                    }
                });
            }

            function createScalabilityChart() {
                const ctx = document.getElementById('scalabilityChart').getContext('2d');
                scalabilityChartInstance = new Chart(ctx, {
                    type: 'line',
                    data: scalabilityData,
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                         plugins: {
                            legend: { position: 'bottom' },
                            title: { display: false },
                        },
                        scales: {
                            y: {
                                beginAtZero: false,
                                min: 54.5,
                                title: { display: true, text: 'Accuracy (%)' }
                            },
                             x: {
                                title: { display: true, text: 'Training Epoch' }
                            }
                        }
                    }
                });
            }

            document.getElementById('task-selector').addEventListener('change', (e) => createResultsChart(e.target.value));
            
            createResultsChart('math');
            createNLUChart();
            createScalabilityChart();
        });
    </script>
</body>
</html>
